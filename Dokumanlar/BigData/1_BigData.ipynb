{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c15a5c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c4dae64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veri seti boyutu: (1000000, 10)\n",
      "Bellek kullanımı: 171.85 MB\n",
      "\n",
      "İlk 5 satır:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>product_id</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "      <th>rating</th>\n",
       "      <th>is_mobile</th>\n",
       "      <th>city</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15796</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>448</td>\n",
       "      <td>Elektronik</td>\n",
       "      <td>55.785318</td>\n",
       "      <td>7</td>\n",
       "      <td>1.782854</td>\n",
       "      <td>True</td>\n",
       "      <td>İzmir</td>\n",
       "      <td>390.497224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>861</td>\n",
       "      <td>2020-01-01 00:01:00</td>\n",
       "      <td>7658</td>\n",
       "      <td>Spor</td>\n",
       "      <td>15.208016</td>\n",
       "      <td>3</td>\n",
       "      <td>2.733450</td>\n",
       "      <td>True</td>\n",
       "      <td>Antalya</td>\n",
       "      <td>45.624049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76821</td>\n",
       "      <td>2020-01-01 00:02:00</td>\n",
       "      <td>3227</td>\n",
       "      <td>Kitap</td>\n",
       "      <td>17.199240</td>\n",
       "      <td>4</td>\n",
       "      <td>1.727572</td>\n",
       "      <td>True</td>\n",
       "      <td>Bursa</td>\n",
       "      <td>68.796962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54887</td>\n",
       "      <td>2020-01-01 00:03:00</td>\n",
       "      <td>8424</td>\n",
       "      <td>Giyim</td>\n",
       "      <td>110.845940</td>\n",
       "      <td>2</td>\n",
       "      <td>1.301790</td>\n",
       "      <td>True</td>\n",
       "      <td>İzmir</td>\n",
       "      <td>221.691880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6266</td>\n",
       "      <td>2020-01-01 00:04:00</td>\n",
       "      <td>3710</td>\n",
       "      <td>Spor</td>\n",
       "      <td>4.096469</td>\n",
       "      <td>4</td>\n",
       "      <td>3.930060</td>\n",
       "      <td>False</td>\n",
       "      <td>Bursa</td>\n",
       "      <td>16.385875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id           timestamp  product_id    category       price  quantity  \\\n",
       "0    15796 2020-01-01 00:00:00         448  Elektronik   55.785318         7   \n",
       "1      861 2020-01-01 00:01:00        7658        Spor   15.208016         3   \n",
       "2    76821 2020-01-01 00:02:00        3227       Kitap   17.199240         4   \n",
       "3    54887 2020-01-01 00:03:00        8424       Giyim  110.845940         2   \n",
       "4     6266 2020-01-01 00:04:00        3710        Spor    4.096469         4   \n",
       "\n",
       "     rating  is_mobile     city  total_amount  \n",
       "0  1.782854       True    İzmir    390.497224  \n",
       "1  2.733450       True  Antalya     45.624049  \n",
       "2  1.727572       True    Bursa     68.796962  \n",
       "3  1.301790       True    İzmir    221.691880  \n",
       "4  3.930060      False    Bursa     16.385875  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_sample_data(n_rows=1000000):\n",
    "    \"\"\"\n",
    "    Örnek büyük veri seti oluşturur\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    data = {\n",
    "        'user_id': np.random.randint(1, 100000, n_rows),\n",
    "        'timestamp': pd.date_range('2020-01-01', periods=n_rows, freq='1min'),\n",
    "        'product_id': np.random.randint(1, 10000, n_rows),\n",
    "        'category': np.random.choice(['Elektronik', 'Giyim', 'Ev&Bahçe', 'Spor', 'Kitap'], n_rows),\n",
    "        'price': np.random.lognormal(3, 1, n_rows),\n",
    "        'quantity': np.random.randint(1, 10, n_rows),\n",
    "        'rating': np.random.uniform(1, 5, n_rows),\n",
    "        'is_mobile': np.random.choice([True, False], n_rows, p=[0.7, 0.3]),\n",
    "        'city': np.random.choice(['İstanbul', 'Ankara', 'İzmir', 'Antalya', 'Bursa'], n_rows)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df['total_amount'] = df['price'] * df['quantity']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 1 milyon kayıtlık örnek veri oluşturalım\n",
    "sample_data = create_sample_data(1000000)\n",
    "\n",
    "print(f\"Veri seti boyutu: {sample_data.shape}\")\n",
    "print(f\"Bellek kullanımı: {sample_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\nİlk 5 satır:\")\n",
    "sample_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a83e0654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orijinal boyut: 171.85 MB\n",
      "Optimize edilmiş boyut: 28.61 MB\n",
      "Tasarruf: 83.4%\n"
     ]
    }
   ],
   "source": [
    "# 1. Bellek optimizasyonu\n",
    "def optimize_dtypes(df):\n",
    "    \"\"\"\n",
    "    DataFrame'in veri tiplerini optimize eder\n",
    "    \"\"\"\n",
    "    optimized_df = df.copy()\n",
    "    \n",
    "    # Integer kolonları optimize et\n",
    "    for col in optimized_df.select_dtypes(include=['int64']).columns:\n",
    "        if optimized_df[col].min() >= 0:\n",
    "            if optimized_df[col].max() < 255:\n",
    "                optimized_df[col] = optimized_df[col].astype('uint8')\n",
    "            elif optimized_df[col].max() < 65535:\n",
    "                optimized_df[col] = optimized_df[col].astype('uint16')\n",
    "            else:\n",
    "                optimized_df[col] = optimized_df[col].astype('uint32')\n",
    "    \n",
    "    # Float kolonları optimize et\n",
    "    for col in optimized_df.select_dtypes(include=['float64']).columns:\n",
    "        optimized_df[col] = optimized_df[col].astype('float32')\n",
    "    \n",
    "    # String kolonları kategori yap\n",
    "    for col in optimized_df.select_dtypes(include=['object']).columns:\n",
    "        if optimized_df[col].nunique() < optimized_df.shape[0] * 0.5:\n",
    "            optimized_df[col] = optimized_df[col].astype('category')\n",
    "    \n",
    "    return optimized_df\n",
    "\n",
    "# Orijinal ve optimize edilmiş veriyi karşılaştıralım\n",
    "original_size = sample_data.memory_usage(deep=True).sum() / 1024**2\n",
    "optimized_data = optimize_dtypes(sample_data)\n",
    "optimized_size = optimized_data.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "print(f\"Orijinal boyut: {original_size:.2f} MB\")\n",
    "print(f\"Optimize edilmiş boyut: {optimized_size:.2f} MB\")\n",
    "print(f\"Tasarruf: {((original_size - optimized_size) / original_size * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5a72189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İşleniyor: Parça 1/10 (100000 kayıt)\n",
      "İşleniyor: Parça 2/10 (100000 kayıt)\n",
      "İşleniyor: Parça 3/10 (100000 kayıt)\n",
      "İşleniyor: Parça 4/10 (100000 kayıt)\n",
      "İşleniyor: Parça 5/10 (100000 kayıt)\n",
      "İşleniyor: Parça 6/10 (100000 kayıt)\n",
      "İşleniyor: Parça 7/10 (100000 kayıt)\n",
      "İşleniyor: Parça 8/10 (100000 kayıt)\n",
      "İşleniyor: Parça 9/10 (100000 kayıt)\n",
      "İşleniyor: Parça 10/10 (100000 kayıt)\n",
      "\n",
      "İşlem süresi: 0.03 saniye\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.pyenv_mirror/user/current/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/tmp/ipykernel_765/2863840371.py:19: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  result = chunk.groupby('category')['total_amount'].sum()\n"
     ]
    }
   ],
   "source": [
    "# 2. Chunked Processing (Parça Parça İşleme)\n",
    "import time\n",
    "\n",
    "def process_in_chunks(df, chunk_size=100000, operation=None):\n",
    "    \"\"\"\n",
    "    Büyük DataFrame'i parçalar halinde işler\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_chunks = len(df) // chunk_size + (1 if len(df) % chunk_size != 0 else 0)\n",
    "    \n",
    "    for i, chunk in enumerate(np.array_split(df, total_chunks)):\n",
    "        print(f\"İşleniyor: Parça {i+1}/{total_chunks} ({len(chunk)} kayıt)\")\n",
    "        \n",
    "        if operation:\n",
    "            result = operation(chunk)\n",
    "            results.append(result)\n",
    "        else:\n",
    "            # Basit aggregation örneği\n",
    "            result = chunk.groupby('category')['total_amount'].sum()\n",
    "            results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Kategori bazında toplam satış miktarını parçalı olarak hesaplayalım\n",
    "start_time = time.time()\n",
    "chunk_results = process_in_chunks(optimized_data)\n",
    "end_time = time.time()\n",
    "\n",
    "# Sonuçları birleştirelim\n",
    "# final_result = sum(chunk_results)\n",
    "print(f\"\\nİşlem süresi: {end_time - start_time:.2f} saniye\")\n",
    "# print(\"\\nKategori bazında toplam satış:\")\n",
    "# print(final_result.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15d857ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop yöntemi: 0.0471 saniye\n",
      "Vectorized yöntem: 0.0007 saniye\n",
      "Hız artışı: 65.4x\n",
      "Sonuçlar eşit: True\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Yavaş yöntem: Loop\n",
    "def calculate_discount_loop(df):\n",
    "    discounts = []\n",
    "    for _, row in df.iterrows():\n",
    "        if row['total_amount'] > 1000:\n",
    "            discount = row['total_amount'] * 0.1\n",
    "        elif row['total_amount'] > 500:\n",
    "            discount = row['total_amount'] * 0.05\n",
    "        else:\n",
    "            discount = 0\n",
    "        discounts.append(discount)\n",
    "    return discounts\n",
    "\n",
    "# Hızlı yöntem: Vectorization\n",
    "def calculate_discount_vectorized(df):\n",
    "    conditions = [\n",
    "        df['total_amount'] > 1000,\n",
    "        df['total_amount'] > 500,\n",
    "    ]\n",
    "    choices = [\n",
    "        df['total_amount'] * 0.1,\n",
    "        df['total_amount'] * 0.05,\n",
    "    ]\n",
    "    return np.select(conditions, choices, default=0)\n",
    "\n",
    "# Küçük bir örneklem ile test edelim (1000 kayıt)\n",
    "test_data = sample_data.head(1000)\n",
    "\n",
    "# Loop yöntemi\n",
    "start = time.time()\n",
    "discount_loop = calculate_discount_loop(test_data)\n",
    "loop_time = time.time() - start\n",
    "\n",
    "# Vectorized yöntem\n",
    "start = time.time()\n",
    "discount_vectorized = calculate_discount_vectorized(test_data)\n",
    "vectorized_time = time.time() - start\n",
    "\n",
    "print(f\"Loop yöntemi: {loop_time:.4f} saniye\")\n",
    "print(f\"Vectorized yöntem: {vectorized_time:.4f} saniye\")\n",
    "print(f\"Hız artışı: {loop_time / vectorized_time:.1f}x\")\n",
    "\n",
    "# Sonuçların aynı olduğunu doğrulayalım\n",
    "print(f\"Sonuçlar eşit: {np.allclose(discount_loop, discount_vectorized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1fdbcb",
   "metadata": {},
   "source": [
    "## Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88bd0984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DASK KURULUMU ===\n",
      "✅ Dask synchronous scheduler ile yapılandırıldı\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 11:37:14,700 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "2025-08-14 11:37:14,704 - distributed.scheduler - INFO - State start\n",
      "2025-08-14 11:37:14,709 - distributed.scheduler - INFO -   Scheduler at:   inproc://10.0.5.2/765/1\n",
      "2025-08-14 11:37:14,710 - distributed.scheduler - INFO -   dashboard at:  http://10.0.5.2:8787/status\n",
      "2025-08-14 11:37:14,711 - distributed.scheduler - INFO - Registering Worker plugin shuffle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 11:37:14,718 - distributed.worker - INFO -       Start worker at:    inproc://10.0.5.2/765/4\n",
      "2025-08-14 11:37:14,719 - distributed.worker - INFO -          Listening to:             inproc10.0.5.2\n",
      "2025-08-14 11:37:14,720 - distributed.worker - INFO -           Worker name:                          0\n",
      "2025-08-14 11:37:14,720 - distributed.worker - INFO -          dashboard at:             10.0.5.2:38537\n",
      "2025-08-14 11:37:14,721 - distributed.worker - INFO - Waiting to connect to:    inproc://10.0.5.2/765/1\n",
      "2025-08-14 11:37:14,722 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-08-14 11:37:14,723 - distributed.worker - INFO -               Threads:                          1\n",
      "2025-08-14 11:37:14,723 - distributed.worker - INFO -                Memory:                 488.28 MiB\n",
      "2025-08-14 11:37:14,724 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iwa8orvm\n",
      "2025-08-14 11:37:14,724 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-08-14 11:37:14,740 - distributed.scheduler - INFO - Register worker addr: inproc://10.0.5.2/765/4 name: 0\n",
      "2025-08-14 11:37:14,741 - distributed.scheduler - INFO - Starting worker compute stream, inproc://10.0.5.2/765/4\n",
      "2025-08-14 11:37:14,741 - distributed.core - INFO - Starting established connection to inproc://10.0.5.2/765/5\n",
      "2025-08-14 11:37:14,742 - distributed.worker - INFO - Starting Worker plugin shuffle\n",
      "2025-08-14 11:37:14,743 - distributed.worker - INFO -         Registered to:    inproc://10.0.5.2/765/1\n",
      "2025-08-14 11:37:14,743 - distributed.worker - INFO - -------------------------------------------------\n",
      "2025-08-14 11:37:14,744 - distributed.core - INFO - Starting established connection to inproc://10.0.5.2/765/1\n",
      "2025-08-14 11:37:14,745 - distributed.scheduler - INFO - Receive client connection: Client-06579da6-7903-11f0-82fd-1a0aab7ed20c\n",
      "2025-08-14 11:37:14,746 - distributed.core - INFO - Starting established connection to inproc://10.0.5.2/765/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dask distributed client başarıyla oluşturuldu\n",
      "<Client: 'inproc://10.0.5.2/765/1' processes=1 threads=1, memory=488.28 MiB>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 11:37:15,020 - distributed.worker.memory - WARNING - Worker is at 175% memory usage. Pausing worker.  Process memory: 856.34 MiB -- Worker memory limit: 488.28 MiB\n",
      "2025-08-14 11:37:15,021 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 856.34 MiB -- Worker memory limit: 488.28 MiB\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "pyarrow>=10.0.1 is required for PyArrow backed StringArray.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Pandas DataFrame'i Dask DataFrame'e dönüştürme\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dask_available:\n\u001b[32m     54\u001b[39m     \u001b[38;5;66;03m# 1. Pandas'dan Dask'a\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     dask_df = \u001b[43mdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnpartitions\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDask DataFrame oluşturuldu: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdask_df.npartitions\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m partition\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     57\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHer partition boyutu: ~\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sample_data)\u001b[38;5;250m \u001b[39m//\u001b[38;5;250m \u001b[39mdask_df.npartitions\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m kayıt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.11/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:4915\u001b[39m, in \u001b[36mfrom_pandas\u001b[39m\u001b[34m(data, npartitions, sort, chunksize)\u001b[39m\n\u001b[32m   4909\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   4910\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease provide chunksize as an int, or possibly as None if you specify npartitions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4911\u001b[39m     )\n\u001b[32m   4913\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataframe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdask_expr\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FromPandas\n\u001b[32m-> \u001b[39m\u001b[32m4915\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mFromPandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4917\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_BackendData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnpartitions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnpartitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpyarrow_strings_enabled\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpyarrow_strings_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.11/lib/python3.12/site-packages/dask/_collections.py:8\u001b[39m, in \u001b[36mnew_collection\u001b[39m\u001b[34m(expr)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_collection\u001b[39m(expr):\n\u001b[32m      7\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create new collection from an expr\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     meta = \u001b[43mexpr\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_meta\u001b[49m\n\u001b[32m      9\u001b[39m     expr._name  \u001b[38;5;66;03m# Ensure backend is imported\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m get_collection_type(meta)(expr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.11/lib/python3.12/functools.py:998\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, instance, owner)\u001b[39m\n\u001b[32m    996\u001b[39m val = cache.get(\u001b[38;5;28mself\u001b[39m.attrname, _NOT_FOUND)\n\u001b[32m    997\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[32m--> \u001b[39m\u001b[32m998\u001b[39m     val = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    999\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1000\u001b[39m         cache[\u001b[38;5;28mself\u001b[39m.attrname] = val\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.11/lib/python3.12/site-packages/dask/dataframe/dask_expr/io/io.py:445\u001b[39m, in \u001b[36mFromPandas._meta\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;129m@functools\u001b[39m.cached_property\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_meta\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pyarrow_strings_enabled:\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m         meta = make_meta(\u001b[43mto_pyarrow_string\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    446\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    447\u001b[39m         meta = \u001b[38;5;28mself\u001b[39m.frame.head(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.11/lib/python3.12/site-packages/dask/dataframe/_pyarrow.py:61\u001b[39m, in \u001b[36m_to_string_dtype\u001b[39m\u001b[34m(df, dtype_check, index_check, string_dtype)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Guards against importing `pyarrow` at the module level (where it may not be installed)\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m string_dtype == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     string_dtype = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mStringDtype\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpyarrow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Possibly convert DataFrame/Series/Index to `string[pyarrow]`\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_dataframe_like(df):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.pyenv_mirror/user/current/lib/python3.12/site-packages/pandas/core/arrays/string_.py:180\u001b[39m, in \u001b[36mStringDtype.__init__\u001b[39m\u001b[34m(self, storage, na_value)\u001b[39m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    177\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStorage must be \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m'\u001b[39m\u001b[33m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstorage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    178\u001b[39m     )\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m storage == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pa_version_under10p1:\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    181\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpyarrow>=10.0.1 is required for PyArrow backed StringArray.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    182\u001b[39m     )\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(na_value, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m np.isnan(na_value):\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# when passed a NaN value, always set to np.nan to ensure we use\u001b[39;00m\n\u001b[32m    186\u001b[39m     \u001b[38;5;66;03m# a consistent NaN value (and we can use `dtype.na_value is np.nan`)\u001b[39;00m\n\u001b[32m    187\u001b[39m     na_value = np.nan\n",
      "\u001b[31mImportError\u001b[39m: pyarrow>=10.0.1 is required for PyArrow backed StringArray."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Dask kurulumu ve temel kullanım\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed\n",
    "import dask\n",
    "\n",
    "# Dask'ı synchronous scheduler ile kullanma (daha güvenli)\n",
    "print(\"=== DASK KURULUMU ===\")\n",
    "try:\n",
    "    # Distributed client yerine synchronous scheduler kullan\n",
    "    dask.config.set(scheduler='synchronous')\n",
    "    print(\"✅ Dask synchronous scheduler ile yapılandırıldı\")\n",
    "    dask_available = True\n",
    "    client = None  # Synchronous modda client gerekmez\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Dask yapılandırılamadı: {e}\")\n",
    "    dask_available = False\n",
    "    client = None\n",
    "\n",
    "# Alternatif: Distributed client deneme (opsiyonel)\n",
    "if dask_available:\n",
    "    try:\n",
    "        from dask.distributed import Client\n",
    "        # Önce mevcut client'ları kapat\n",
    "        try:\n",
    "            from dask.distributed import default_client\n",
    "            old_client = default_client()\n",
    "            old_client.close()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Yeni client oluştur - daha konservatif ayarlarla\n",
    "        client = Client(\n",
    "            processes=False,  # Thread-based workers kullan (daha stabil)\n",
    "            threads_per_worker=1, \n",
    "            n_workers=1,  # Tek worker ile başla\n",
    "            memory_limit='512MB',  # Daha düşük memory limit\n",
    "            silence_logs=True,\n",
    "            dashboard_address=None  # Dashboard'u devre dışı bırak\n",
    "        )\n",
    "        print(f\"✅ Dask distributed client başarıyla oluşturuldu\")\n",
    "        print(client)\n",
    "        dask.config.set(scheduler='distributed')\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Distributed client oluşturulamadı: {e}\")\n",
    "        print(\"Synchronous scheduler kullanılacak...\")\n",
    "        dask.config.set(scheduler='synchronous')\n",
    "        client = None\n",
    "\n",
    "# %%\n",
    "# Pandas DataFrame'i Dask DataFrame'e dönüştürme\n",
    "\n",
    "if dask_available:\n",
    "    # 1. Pandas'dan Dask'a\n",
    "    dask_df = dd.from_pandas(sample_data, npartitions=4)\n",
    "    print(f\"Dask DataFrame oluşturuldu: {dask_df.npartitions} partition\")\n",
    "    print(f\"Her partition boyutu: ~{len(sample_data) // dask_df.npartitions:,} kayıt\")\n",
    "    \n",
    "    # 2. Dosyadan direkt okuma (daha verimli)\n",
    "    # Önce veriyi kaydetelim\n",
    "    sample_data.to_csv('sample_data.csv', index=False)\n",
    "    dask_df_from_file = dd.read_csv('sample_data.csv')\n",
    "    \n",
    "    print(f\"\\nDosyadan okunan Dask DataFrame: {dask_df_from_file.npartitions} partition\")\n",
    "else:\n",
    "    print(\"⚠️ Dask kullanılamıyor, bu bölüm atlanıyor...\")\n",
    "\n",
    "# %%\n",
    "# Dask ile temel operasyonlar\n",
    "\n",
    "if dask_available:\n",
    "    # 1. Lazy evaluation örneği\n",
    "    print(\"=== LAZY EVALUATION ÖRNEĞİ ===\")\n",
    "    \n",
    "    # Bu işlemler hemen çalışmaz, sadece task graph oluşturur\n",
    "    filtered_data = dask_df[dask_df['total_amount'] > 100]\n",
    "    grouped_data = filtered_data.groupby('category')['total_amount'].mean()\n",
    "    \n",
    "    print(\"İşlemler tanımlandı, henüz çalıştırılmadı...\")\n",
    "    \n",
    "    # compute() ile işlemleri çalıştıralım\n",
    "    start_time = time.time()\n",
    "    result = grouped_data.compute()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"İşlem süresi: {end_time - start_time:.2f} saniye\")\n",
    "    print(\"\\nSonuç:\")\n",
    "    print(result.sort_values(ascending=False))\n",
    "else:\n",
    "    print(\"⚠️ Dask kullanılamıyor, bu bölüm atlanıyor...\")\n",
    "\n",
    "# %%\n",
    "# Dask vs Pandas performans karşılaştırması\n",
    "\n",
    "if dask_available:\n",
    "    print(\"=== PERFORMANS KARŞILAŞTIRMASI ===\")\n",
    "    \n",
    "    # Kompleks bir aggregation işlemi tanımlayalım\n",
    "    def complex_aggregation_pandas(df):\n",
    "        return df.groupby(['category', 'city']).agg({\n",
    "            'total_amount': ['sum', 'mean', 'count'],\n",
    "            'rating': 'mean',\n",
    "            'quantity': 'sum'\n",
    "        })\n",
    "    \n",
    "    def complex_aggregation_dask(df):\n",
    "        return df.groupby(['category', 'city']).agg({\n",
    "            'total_amount': ['sum', 'mean', 'count'],\n",
    "            'rating': 'mean',\n",
    "            'quantity': 'sum'\n",
    "        }).compute()\n",
    "    \n",
    "    # Pandas ile test\n",
    "    start = time.time()\n",
    "    pandas_result = complex_aggregation_pandas(sample_data)\n",
    "    pandas_time = time.time() - start\n",
    "    \n",
    "    # Dask ile test\n",
    "    start = time.time()\n",
    "    dask_result = complex_aggregation_dask(dask_df)\n",
    "    dask_time = time.time() - start\n",
    "    \n",
    "    print(f\"Pandas süresi: {pandas_time:.2f} saniye\")\n",
    "    print(f\"Dask süresi: {dask_time:.2f} saniye\")\n",
    "    print(f\"Hız karşılaştırması: {pandas_time/dask_time:.1f}x {'(Dask daha hızlı)' if dask_time < pandas_time else '(Pandas daha hızlı)'}\")\n",
    "    \n",
    "    print(f\"\\nSonuç boyutu: {pandas_result.shape}\")\n",
    "    print(\"İlk 5 satır:\")\n",
    "    print(pandas_result.head())\n",
    "else:\n",
    "    print(\"⚠️ Dask kullanılamıyor, performans karşılaştırması atlanıyor...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a20e3092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Downloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m159.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "Successfully installed pyarrow-21.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0310b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
